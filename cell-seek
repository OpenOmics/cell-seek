#!/usr/bin/env python3
# -*- coding: UTF-8 -*-

"""
ABOUT: This is the main entry for the pipeline.
REQUIRES:
  - python>=3.6
  - snakemake   (recommended>=6.0.0)
  - singularity (recommended==latest)
DISCLAIMER:
                    PUBLIC DOMAIN NOTICE
        NIAID Collaborative Bioinformatics Resource (NCBR)
   National Institute of Allergy and Infectious Diseases (NIAID)
This software/database is a "United  States Government Work" under
the terms of the United  States Copyright Act.  It was written as
part of the author's official duties as a United States Government
employee and thus cannot be copyrighted. This software is freely
available to the public for use.
Although all  reasonable  efforts have been taken  to ensure  the
accuracy and reliability of the software and data, NCBR do not and
cannot warrant the performance or results that may  be obtained by
using this software or data. NCBR and NIH disclaim all warranties,
express  or  implied,  including   warranties   of   performance,
merchantability or fitness for any particular purpose.
Please cite the author and NIH resources like the "Biowulf Cluster"
in any work or product based on this material.
USAGE:
  $ cell-seek <command> [OPTIONS]
EXAMPLE:
  $ cell-seek run --input *.R?.fastq.gz --output output --version gex --genome hg38/
"""

# Python standard library
from __future__ import print_function
import sys, os, subprocess, re, json, textwrap

# 3rd party imports from pypi
import argparse  # potential python3 3rd party package, added in python/3.5

# Local imports
from src import version
from src.run import init, setup, bind, dryrun, runner
from src.shells import bash
from src.utils import (
    Colors,
    err,
    exists,
    fatal,
    hashed,
    permissions,
    check_cache,
    require
)


# Pipeline Metadata
__version__ = version
__authors__ = 'Skyler Kuhn, Vicky Chen'
__email__ = 'skyler.kuhn@nih.gov, vicky.chen@nih.gov'
__home__  =  os.path.dirname(os.path.abspath(__file__))
_name = os.path.basename(sys.argv[0])
_description = 'One single-cell pipeline to rule them all!'


def unlock(sub_args):
    """Unlocks a previous runs output directory. If snakemake fails ungracefully,
    it maybe required to unlock the working directory before proceeding again.
    This is rare but it does occasionally happen. Maybe worth add a --force
    option to delete the '.snakemake/' directory in the future.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for unlock sub-command
    """
    print("Unlocking the pipeline's output directory...")
    outdir = sub_args.output

    try:
        unlock_output = subprocess.check_output([
            'snakemake', '--unlock',
            '--cores', '1',
            '--configfile=config.json'
        ], cwd = outdir,
        stderr=subprocess.STDOUT)
    except subprocess.CalledProcessError as e:
        # Unlocking process returned a non-zero exit code
        sys.exit("{}\n{}".format(e, e.output))

    print("Successfully unlocked the pipeline's working directory!")


def run(sub_args):
    """Initialize, setup, and run the pipeline.
    Calls initialize() to create output directory and copy over pipeline resources,
    setup() to create the pipeline config file, dryrun() to ensure their are no issues
    before running the pipeline, and finally run() to execute the Snakemake workflow.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for run sub-command
    """
    # Step 0. Check for required dependencies
    # The pipelines has only two requirements:
    # snakemake and singularity
    require(['snakemake', 'singularity'], ['snakemake', 'singularity'])

    # Step 1. Initialize working directory,
    # copy over required resources to run
    # the pipeline
    git_repo = __home__
    input_files = init(
        repo_path = git_repo,
        output_path = sub_args.output,
        links = sub_args.input,
        sym_link = False
    )

    # Step 2. Setup pipeline for execution,
    # dynamically create config.json config
    # file from user inputs and base config
    # templates
    config = setup(sub_args,
        ifiles = input_files,
        repo_path = git_repo,
        output_path = sub_args.output
    )

    # Step 3. Resolve docker/singularity bind
    # paths from the config file.
    bindpaths = bind(
        sub_args,
        config = config
    )

    config['bindpaths'] = bindpaths

    # Step 4. Save config to output directory
    with open(os.path.join(sub_args.output, 'config.json'), 'w') as fh:
        json.dump(config, fh, indent = 4, sort_keys = True)

    # Optional Step: Dry-run pipeline
    if sub_args.dry_run:
        # Dryrun pipeline
        dryrun_output = dryrun(outdir = sub_args.output) # python3 returns byte-string representation
        print("\nDry-running {} pipeline:\n{}".format(_name, dryrun_output.decode("utf-8")))
        sys.exit(0)

    # Step 5. Orchestrate pipeline execution,
    # run pipeline in locally on a compute node
    # for debugging purposes or submit the master
    # job to the job scheduler, SLURM, and create
    # logging file
    if not exists(os.path.join(sub_args.output, 'logfiles')):
        # Create directory for logfiles
        os.makedirs(os.path.join(sub_args.output, 'logfiles'))
    if sub_args.mode == 'local':
        log = os.path.join(sub_args.output, 'logfiles', 'snakemake.log')
    else:
        log = os.path.join(sub_args.output, 'logfiles', 'master.log')
    logfh = open(log, 'w')
    mjob = runner(mode = sub_args.mode,
        outdir = sub_args.output,
        # additional_bind_paths = all_bind_paths,
        alt_cache = sub_args.singularity_cache,
        threads = int(sub_args.threads),
        jobname = sub_args.job_name,
        submission_script=os.path.join(__home__, 'src', 'run.sh'),
        logger = logfh,
        additional_bind_paths = ",".join(bindpaths),
        tmp_dir = sub_args.tmp_dir,
    )

    # Step 6. Wait for subprocess to complete,
    # this is blocking and not asynchronous
    if not sub_args.silent:
        print("\nRunning {} pipeline in '{}' mode...".format(_name, sub_args.mode))
    mjob.wait()
    logfh.close()

    # Step 7. Relay information about submission
    # of the master job or the exit code of the
    # pipeline that ran in local mode
    if sub_args.mode == 'local':
        if int(mjob.returncode) == 0:
            print('{} pipeline has successfully completed'.format(_name))
        else:
            fatal('{} pipeline failed. Please see {} for more information.'.format(_name,
                os.path.join(sub_args.output, 'logfiles', 'snakemake.log')))
    elif sub_args.mode == 'slurm':
        jobid = open(os.path.join(sub_args.output, 'logfiles', 'mjobid.log')).read().strip()
        if not sub_args.silent:
            if int(mjob.returncode) == 0:
                print('Successfully submitted master job: ', end="")
            else:
                fatal('Error occurred when submitting the master job.')
        print(jobid)


def cache(sub_args):
    """Caches remote resources or reference files stored on DockerHub and S3.
    Local SIFs will be created from images defined in 'config/containers/images.json'.
    @TODO: add option to cache other shared S3 resources (i.e. kraken db and fqscreen indices)
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for unlock sub-command
    """
    print(sub_args)
    fatal('NotImplementedError... Comming Soon!')


def parsed_arguments(name, description):
    """Parses user-provided command-line arguments. Requires argparse and textwrap
    package. argparse was added to standard lib in python 3.5 and textwrap was added
    in python 3.5. To create custom help formatting for subparsers a docstring is
    used create the help message for required options. argparse does not support named
    subparser groups, which is normally what would be used to accomphish this reformatting.
    As so, the help message for require options must be suppressed. If a new required arg
    is added to a subparser, it must be added to the docstring and the usage statement
    also must be updated.
    @param name <str>:
        Name of the pipeline or command-line tool
    @param description <str>:
        Short description of pipeline or command-line tool
    """
    # Add styled name and description
    c = Colors
    styled_name = "{0}{1}{2}cell-seek{3}".format(c.bold, c.bg_black, c.cyan, c.end)
    description = "{0}{1}{2}".format(c.bold, description, c.end)

    # Create a top-level parser
    parser = argparse.ArgumentParser(description = '{}: {}'.format(styled_name, description))

    # Adding Verison information
    parser.add_argument('--version', action = 'version', version='%(prog)s {}'.format(__version__))

    # Create sub-command parser
    subparsers = parser.add_subparsers(help='List of available sub-commands')

    # Sub-parser for the "run" sub-command
    # Grouped sub-parser arguments are currently
    # not supported: https://bugs.python.org/issue9341
    # Here is a work around to create more useful help message for named
    # options that are required! Please note: if a required arg is added the
    # description below should be updated (i.e. update usage and add new option)
    required_run_options = textwrap.dedent("""\
        {0}: {1}

        {3}{4}Synopsis:{5}
          $ {2} run [--help] \\
                [--dry-run] [--job-name JOB_NAME] [--mode {{slurm,local}}] \\
                [--sif-cache SIF_CACHE] [--singularity-cache SINGULARITY_CACHE] \\
                [--silent] [--threads THREADS] [--tmp-dir TMP_DIR] \\
                [--aggregate {{mapped, none}}][--libraries LIBRARIES] \\
                [--features FEATURES] [--cmo-reference CMOREFERENCE] \\
                [--cmo-sample CMOSAMPLE] [--exclude-introns] [--filter FILTER] \\
                [--create-bam] \\
                --input INPUT [INPUT ...] \\
                --output OUTPUT \\
                --version {{gex, ...}} \\
                --genome {{hg38, ...}}

        Optional arguments are shown in square brackets above.

        {3}{4}Description:{5}
          To run the cell-seek pipeline with your data raw data, please
        provide a space seperated list of FastQ (globbing is supported) and an output
        directory to store results.

        {3}{4}Required arguments:{5}
          --input INPUT [INPUT ...]
                                Input FastQ file(s) to process. The pipeline does NOT
                                support single-end data. FastQ files for one or more
                                samples can be provided. Multiple input FastQ files
                                should be seperated by a space. Globbing for multiple
                                file is also supported.
                                  Example: --input .tests/*.R?.fastq.gz
          --output OUTPUT
                                Path to an output directory. This location is where
                                the pipeline will create all of its output files, also
                                known as the pipeline's working directory. If the user
                                provided working directory has not been initialized,
                                it will be created automatically.
                                  Example: --output /data/$USER/output
          --version {{gex,vdj,cite,multi,atac}}
                                The version of the pipeline to run. Can be chosen from
                                the options gene expression (GEX), VDJ, feature
                                barcode (CITE), Cell Ranger multi (Multi), or ATAC.
                                  Example: --version gex
          --genome  {{hg38,mm10}}
                                Reference genome. This option defines the reference
                                genome of the samples. {2} does comes bundled with
                                prebuilt reference files for human and mouse samples,
                                e.g. hg38 or mm10. Please select one of the following
                                options: hg38, mm10.
                                  Example: --genome hg38
        {3}{4}Analysis options:{5}
          --aggregate  {{mapped,none}}
                                Cell Ranger aggregate. This option defines the
                                normalization mode that should be used. Mapped is what
                                Cell Ranger would run by default, which subsamples reads
                                from higher depth samples until each library type has an
                                equal number of reads per cell that are confidently mapped.
                                None means to not normalize at all. If this flag is not
                                used then aggregate will not be run. To run Cell Ranger
                                aggregate, please select one of the following options:
                                mapped, none.
                                  Example: --aggregate mapped
          --libraries LIBRARIES
                                Libraries file. A CSV file containing information about
                                each library.  This file is used in feature barcode (cite),
                                multi, and multiome analysis.It contains each sample's
                                name, flowcell, demultiplexed name, and library type.
                                  Here is an example libraries.csv file:
                                    Name,Flowcell,Sample,Type
                                    IL15_LNs,H7CNNBGXG,IL15_LNs,Gene Expression
                                    IL15_LNs,H7CT7BGXG,IL15_LNs_BC,Antibody Capture
                                  where:
                                    • Name: name of the sample passed to CellRanger.
                                    • Flowcell: the flowcell that contains the FASTQ
                                      files for this set of data.
                                    • Sample: name that was used when demultiplexing,
                                      this should match the FASTQ files.
                                    • Type: library type for each sample. List of
                                      supported options:
                                        • Gene Expression
                                        • CRISPR Guide Capture,
                                        • Antibody Capture
                                        • Custom
                                  Example: --libraries libraries.csv
          --features FEATURES
                                Features file. A feature reference CSV file containing
                                information for processing a feature barcode data. This
                                file should contain a unique ID for the feature, a human
                                readable name, sequence, feature type, read, and pattern.
                                  Here is an example features.csv file:
                                    id,name,sequence,feature_type,read,pattern
                                    CITE_CD64,CD64,AGTGGG,Antibody Capture,R2,5PNN(BC)
                                    CITE_CD8,CD8,TCACCGT,Antibody Capture,R2,5PNNN(BC)
                                  where:
                                    • id: Unique ID for this feature. Must not contain
                                      whitespace, quote or comma characters. Each ID
                                      must be unique and must not collide with a gene
                                      identifier from the transcriptome.
                                    • name: Human-readable name for this feature. Must
                                      not contain whitespace.
                                    • sequence: Nucleotide barcode sequence associated
                                      with this feature, e.g. the antibody barcode or
                                      sgRNA protospacer sequence.
                                    • read: Specifies which RNA sequencing read contains
                                      the Feature Barcode sequence. Must be R1 or R2, but
                                      in most cases R2 is the correct read.
                                    • pattern: Specifies how to extract the sequence of
                                      the feature barcode from the read.
                                    • Type of the feature. List of supported options:
                                        • Gene Expression
                                        • CRISPR Guide Capture,
                                        • Antibody Capture
                                        • Custom
                                  Example: --features features.csv
          --cmo-reference CMOREFERENCE
                                CMO reference file. A CMO reference CSV file containing
                                information for processing hashtag data, which is used in
                                multi analysis if custom hashtags will be processed. This
                                file should contain a unique ID for the hashtag, a human
                                readable name, sequence, feature type, read, and pattern.
                                  Here is an example cmo_reference.csv file:
                                    id,name,sequence,feature_type,read,pattern
                                    CMO301,CMO301,ATGAGGAATTCCTGC,Multiplexing Capture,R2,5P(BC)
                                    CMO302,CMO302,CATGCCAATAGAGCG,Multiplexing Capture,R2,5P(BC)
                                  where:
                                    • id: Unique ID for this feature. Must not contain
                                      whitespace, quote or comma characters. Each ID
                                      must be unique and must not collide with a gene
                                      identifier from the transcriptome.
                                    • name: Human-readable name for this feature. Must
                                      not contain whitespace.
                                    • sequence: Nucleotide barcode sequence associated
                                      with this hashtag.
                                    • feature_type: Type of the feature. This should always be
                                      multiplexing capture.
                                    • read: Specifies which RNA sequencing read contains
                                      the Feature Barcode sequence. Must be R1 or R2, but
                                      in most cases R2 is the correct read.
                                    • pattern: Specifies how to extract the sequence of
                                      the feature barcode from the read.
                                    • Type of the feature. This should always be Multiplexing Capture
                                  Example: --cmo-reference cmo_reference.csv
          --cmo-sample CMOSAMPLE
                                CMO sample file. A CMO sample CSV file containing
                                sample to hashtag information used for multi analysis.
                                CMO IDs should match the ones used in the CMO reference
                                file. If no CMO reference is provided then CMO ID should
                                match the ones used by 10x. The same CMO sample will be
                                used on all multi libraries. This file should contain a
                                unique sample ID for the sample, and the CMO IDs associated
                                with that sample. If more than one CMO ID is associated with
                                a sample then a | should be used to separate the tags.
                                  Here is an example cmo_sample.csv file:
                                    sample_id,cmo_ids
                                    sample1,CMO301
                                    sample2,CMO302|CMO303
                                  where:
                                    • sample_id: Unique sample ID for this hashtagged sample.
                                      Must not contain, whitespace, quote or comma characters.
                                      Each sample ID must be unique.
                                    • cmo_ids: Unique CMO ID(s) that the sample is hashtagged
                                      with. Must match either entries in cmo_reference.csv file
                                      or 10x CMO IDs.
                                  Example: --cmo-sample cmo_sample.csv
          --exclude-introns
                                Exclude introns from the count alignment. This flag is
                                only applicable when dealing with gene expression related
                                data.
                                  Example: --exclude-introns
          --filter FILTER
                                Filter threshold file. A CSV file containing the different
                                thresholds to be applied for individual samples within the
                                project during the QC analysis. The file should contain a
                                header row with Sample as the column name for the sample IDs,
                                and the name of each metric that will be filtered along with
                                if it is the high or low threshold for that metric. Each row
                                is then the entries for each sample that the manual thresholds
                                will be applied. If no file is provided then the default
                                thresholds will be used. If a cell is left blank for a sample
                                then that sample would not be filtered based on that criteria.
                                This flag is currently only applicable when dealing with GEX
                                projects.
                                  Here is an example filter.csv file:
                                    Sample,nFeature_RNA_low,nFeature_RNA_high,percent.mito_high
                                    sample1,500,6000,15
                                    sample2,500,6000,5
                                    sample4,500,6000,5
                                  where:
                                    • Sample: Unique sample ID that should match the sample name
                                      used for Cell Ranger count.
                                    • nFeature_RNA_low,nFeature_RNA_high,percent.mito_high: Example
                                      entries that can be used for manual thresholding. The column
                                      names need to be formatted as metadataname_high/low. Entries
                                      that ends with high will be treated as the upper threshold.
                                      Entries that ends with low will be treated as the lower
                                      threshold. Valid metadata names include nCount_RNA,
                                      nFeature_RNA, and percent.mito.
                                  Example: --filter filter.csv
          --create-bam
                                Create bam files. By default the no-bam flag is used when running
                                Cell Ranger. Use this flag to ensure that a bam file is created for
                                each sample during analysis. This flag is only applicable when
                                dealing with gene expression related data.
                                  Example: --create-bam

        {3}{4}Orchestration options:{5}
          --mode {{slurm,local}}
                                Method of execution. Defines the mode of execution.
                                Vaild options for this mode include: local or slurm.
                                Additional modes of exection are coming soon, default:
                                slurm.
                                Here is a brief description of each mode:
                                   • local: uses local method of execution. local runs
                                will run serially on compute instance. This is useful
                                for testing, debugging, or when a users does not have
                                access to a  high  performance  computing environment.
                                If this option is not provided, it will default to a
                                slurm mode of execution.
                                   • slurm: uses slurm execution backend. This method
                                will submit jobs to a  cluster  using sbatch. It is
                                recommended running the pipeline in this mode as it
                                will be significantly faster.
                                  Example: --mode slurm
          --job-name JOB_NAME
                                Overrides the name of the pipeline's master job. When
                                submitting the pipeline to a jobscheduler, this option
                                overrides the default name of the master job. This can
                                be useful for tracking the progress or status of a run,
                                default: pl:{2}.
                                  Example: --job-name {2}_03-14.1592
          --dry-run
                                Does not execute anything. Only displays what steps in
                                the pipeline remain or will be run.
                                  Example: --dry-run
          --silent
                                Silence standard output. This will reduces the amount
                                of information displayed to standard  output  when the
                                master job is submitted to the job scheduler. Only the
                                job id of the master job is returned.
                                  Example: --silent
          --singularity-cache SINGULARITY_CACHE
                                Overrides the $SINGULARITY_CACHEDIR variable. Images
                                from remote registries are cached locally on the file
                                system. By default, the singularity cache is set to:
                                '/path/to/output/directory/.singularity/'. Please note
                                that this cache cannot be shared across users.
                                  Example: --singularity-cache /data/$USER
          --sif-cache SIF_CACHE
                                Path where a local cache of SIFs are stored. This cache
                                can be shared across users if permissions are properly
                                setup. If a SIF does not exist in the SIF cache, the
                                image will be pulled from Dockerhub. {2} cache
                                sub command can be used to create a local SIF cache.
                                Please see {2} cache for more information.
                                   Example: --sif-cache /data/$USER/sifs/
          --tmp-dir TMP_DIR
                                Path on the file system for writing temporary output
                                files. By default, the temporary directory is set to
                                '/lscratch/$SLURM_JOBID' for backwards compatibility
                                with the NIH's Biowulf cluster; however, if you are
                                running the pipeline on another cluster, this option
                                will need to be specified. Ideally, this path should
                                point to a dedicated location on the filesystem for
                                writing tmp files. On many systems, this location is
                                set to somewhere in /scratch. If you need to inject a
                                variable into this string that should NOT be expanded,
                                please quote this options value in single quotes.
                                  Example: --tmp-dir '/scratch/$USER/'
          --threads THREADS
                                Max number of threads for local processes. It is
                                recommended setting this vaule to the maximum number
                                of CPUs available on the host machine, default: 2.
                                  Example: --threads: 16

        {3}{4}Misc Options:{5}
          -h, --help            Show usage information, help message, and exit.
                                  Example: --help
        """.format(styled_name, description, name, c.bold, c.url, c.end, c.italic))

    # Display example usage in epilog
    run_epilog = textwrap.dedent("""\
        {2}{3}Example:{4}
          # Step 1.) Grab an interactive node,
          # do not run on head node!
          srun -N 1 -n 1 --time=1:00:00 --mem=8gb  --cpus-per-task=2 --pty bash
          module purge
          module load singularity snakemake

          # Step 2A.) Dry-run the pipeline
          ./{0} run --input .tests/*.R?.fastq.gz \\
                         --output /data/$USER/output \\
                         --version gex \\
                         --genome hg38 \\
                         --mode slurm \\
                         --dry-run

          # Step 2B.) Run the {0} pipeline
          # The slurm mode will submit jobs to
          # the cluster. It is recommended running
          # the pipeline in this mode.
          ./{0} run --input .tests/*.R?.fastq.gz \\
                         --output /data/$USER/output \\
                         --version gex \\
                         --genome hg38 \\
                         --mode slurm

        {2}{3}Version:{4}
          {1}
        """.format(name, __version__, c.bold, c.url, c.end))

    # Supressing help message of required args to overcome no sub-parser named groups
    subparser_run = subparsers.add_parser('run',
        help = 'Run the {} pipeline with input files.'.format(name),
        usage = argparse.SUPPRESS,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description = required_run_options,
        epilog  = run_epilog,
        add_help=False
    )

    # Required Arguments
    # Input FastQ files
    subparser_run.add_argument(
        '--input',
        # Check if the file exists and if it is readable
        type = lambda file: permissions(parser, file, os.R_OK),
        required = True,
        nargs = '+',
        help = argparse.SUPPRESS
    )

    # Output Directory, i.e
    # working directory
    subparser_run.add_argument(
        '--output',
        type = lambda option: os.path.abspath(os.path.expanduser(option)),
        required = True,
        help = argparse.SUPPRESS
    )

    # Version of pipeline to run
    # i.e. GEX, VDJ, Multi, etc.
    subparser_run.add_argument(
        '--version',
        type = str.lower,
        required = True,
        default = "gex",
        choices = ['gex', 'cite', 'multi', 'vdj', 'atac', 'multiome'],
        help = argparse.SUPPRESS
    )

    # Execution Method, run locally
    # on a compute node, submit to
    # SLURM job scheduler, etc.
    subparser_run.add_argument(
        '--genome',
        type = str,
        required = True,
        choices = ['hg38', 'mm10'],
        help = argparse.SUPPRESS
    )

    # Optional Arguments
    # Add custom help message
    subparser_run.add_argument(
        '-h', '--help',
        action='help',
        help=argparse.SUPPRESS
    )

    # Analysis options
    # Input Libraries files for
    # cellranger count
    subparser_run.add_argument(
        '--libraries',
        # Check if the file exists and if it is readable
        type = lambda file: permissions(parser, file, os.R_OK),
        required = False,
        help = argparse.SUPPRESS
    )

    # Input Libraries files
    subparser_run.add_argument(
        '--features',
        # Check if the file exists and if it is readable
        type = lambda file: permissions(parser, file, os.R_OK),
        required = False,
        help = argparse.SUPPRESS
    )

    # Input CMO Reference files
    subparser_run.add_argument(
        '--cmo-reference',
        # Check if the file exists and if it is readable
        type = lambda file: permissions(parser, file, os.R_OK),
        required = False,
        help = argparse.SUPPRESS
    )

    # Input CMO Sample files
    subparser_run.add_argument(
        '--cmo-sample',
        # Check if the file exists and if it is readable
        type = lambda file: permissions(parser, file, os.R_OK),
        required = False,
        help = argparse.SUPPRESS
    )

    # Exclude introns from count
    subparser_run.add_argument(
        '--exclude-introns',
        action = 'store_true',
        required = False,
        default = False,
        help = argparse.SUPPRESS
    )

    # How to run Cell Ranger aggregate
    subparser_run.add_argument(
        '--aggregate',
        type = str.lower,
        required = False,
        default = "",
        choices = ['none', 'mapped'],
        help = argparse.SUPPRESS
    )

    # Thresholds to use for filtering in QC Analysis
    subparser_run.add_argument(
        '--filter',
        # Check if the file exists and if it is readable
        type = lambda file: permissions(parser, file, os.R_OK),
        required = False,
        help = argparse.SUPPRESS
    )

    # Create BAM file during run
    subparser_run.add_argument(
        '--create-bam',
        action = 'store_true',
        required = False,
        default = False,
        help = argparse.SUPPRESS
    )

    # Orchestration Options
    # Execution Method, run locally
    # on a compute node or submit to
    # a supported job scheduler, etc.
    subparser_run.add_argument(
        '--mode',
        type = str,
        required = False,
        default = "slurm",
        choices = ['slurm', 'local'],
        help = argparse.SUPPRESS
    )

    # Name of master job
    subparser_run.add_argument(
        '--job-name',
        type = str,
        required = False,
        default = 'pl:{}'.format(name),
        help = argparse.SUPPRESS
    )

    # Dry-run
    # Do not execute the workflow,
    # prints what steps remain
    subparser_run.add_argument(
        '--dry-run',
        action = 'store_true',
        required = False,
        default = False,
        help = argparse.SUPPRESS
    )

    # Silent output mode
    subparser_run.add_argument(
        '--silent',
        action = 'store_true',
        required = False,
        default = False,
        help = argparse.SUPPRESS
    )

    # Singularity cache directory,
    # default uses output directory
    subparser_run.add_argument(
        '--singularity-cache',
        type = lambda option: check_cache(parser, os.path.abspath(os.path.expanduser(option))),
        required = False,
        help = argparse.SUPPRESS
    )

    # Local SIF cache directory,
    # default pull from Dockerhub
    subparser_run.add_argument(
        '--sif-cache',
        type = lambda option: os.path.abspath(os.path.expanduser(option)),
        required = False,
        help = argparse.SUPPRESS
    )

    # Base directory to write
    # temporary/intermediate files
    subparser_run.add_argument(
        '--tmp-dir',
        type = str,
        required = False,
        default = '/lscratch/$SLURM_JOBID/',
        help = argparse.SUPPRESS
    )

    # Number of threads for the
    # pipeline's main proceess
    # This is only applicable for
    # local rules or when running
    # in local mode.
    subparser_run.add_argument(
        '--threads',
        type = int,
        required = False,
        default = 2,
        help = argparse.SUPPRESS
    )

    # Sub-parser for the "unlock" sub-command
    # Grouped sub-parser arguments are currently
    # not supported: https://bugs.python.org/issue9341
    # Here is a work around to create more useful help message for named
    # options that are required! Please note: if a required arg is added the
    # description below should be updated (i.e. update usage and add new option)
    required_unlock_options = textwrap.dedent("""\
        {0}: {1}

        {3}{4}Synopsis:{5}
          $ {2} unlock [-h] --output OUTPUT

        Optional arguments are shown in square brackets above.

        {3}{4}Description:{5}
          If the pipeline fails ungracefully, it maybe required to unlock
        the working directory before proceeding again. Please verify that
        the pipeline is not running before running this command. If the
        pipeline is still running, the workflow manager will report the
        working directory is locked. This is normal behavior. Do NOT run
        this command if the pipeline is still running.

        {3}{4}Required arguments:{5}
          --output OUTPUT       Path to a previous run's output directory
                                to unlock. This will remove a lock on the
                                working directory. Please verify that the
                                pipeline is not running before running
                                this command.
                                  Example: --output /data/$USER/output

        {3}{4}Misc Options:{5}
          -h, --help            Show usage information, help message,
                                and exit.
                                  Example: --help
        """.format(styled_name, description, name, c.bold, c.url, c.end))

    # Display example usage in epilog
    unlock_epilog = textwrap.dedent("""\
        {2}{3}Example:{4}
          # Unlock output directory of pipeline
          {0} unlock --output /data/$USER/output

        {2}{3}Version:{4}
          {1}
        """.format(name, __version__, c.bold, c.url, c.end))

    # Supressing help message of required args to overcome no sub-parser named groups
    subparser_unlock = subparsers.add_parser(
        'unlock',
        help = 'Unlocks a previous runs output directory.',
        usage = argparse.SUPPRESS,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description = required_unlock_options,
        epilog = unlock_epilog,
        add_help = False
    )

    # Required Arguments
    # Output Directory (analysis working directory)
    subparser_unlock.add_argument(
        '--output',
        type = str,
        required = True,
        help = argparse.SUPPRESS
    )

    # Add custom help message
    subparser_unlock.add_argument(
        '-h', '--help',
        action='help',
        help=argparse.SUPPRESS
    )


    # Sub-parser for the "cache" sub-command
    # Grouped sub-parser arguments are
    # not supported: https://bugs.python.org/issue9341
    # Here is a work around to create more useful help message for named
    # options that are required! Please note: if a required arg is added the
    # description below should be updated (i.e. update usage and add new option)
    required_cache_options = textwrap.dedent("""\
        {0}: {1}

        {3}{4}Synopsis:{5}
          $ {2} cache [-h] [--dry-run] --sif-cache SIF_CACHE

        Optional arguments are shown in square brackets above.

        {3}{4}Description:{5}
          Creates a local cache resources hosted on DockerHub or AWS S3.
        These resources are normally pulled onto the filesystem when the
        pipeline runs; however, due to network issues or DockerHub pull
        rate limits, it may make sense to pull the resources once so a
        shared cache can be created. It is worth noting that a singularity
        cache cannot normally be shared across users. Singularity strictly
        enforces that a cache is owned by the user. To get around this
        issue, the cache subcommand can be used to create local SIFs on
        the filesystem from images on DockerHub.

        {3}{4}Required arguments:{5}
          --sif-cache SIF_CACHE
                                Path where a local cache of SIFs will be
                                stored. Images defined in containers.json
                                will be pulled into the local filesystem.
                                The path provided to this option can be
                                passed to the --sif-cache option of the
                                run sub command. Please see {2}
                                run sub command for more information.
                                  Example: --sif-cache /data/$USER/cache

        {3}{4}Orchestration options:{5}
          --dry-run             Does not execute anything. Only displays
                                what remote resources would be pulled.
                                  Example: --dry-run

        {3}{4}Misc Options:{5}
          -h, --help            Show usage information, help message,
                                and exits.
                                  Example: --help

        """.format(styled_name, description, name, c.bold, c.url, c.end))

    # Display example usage in epilog
    cache_epilog = textwrap.dedent("""\
        {2}{3}Example:{4}
          # Cache remote resources of pipeline
          {0} cache --sif-cache /data/$USER/cache

        {2}{3}Version:{4}
          {1}
        """.format(name, __version__, c.bold, c.url, c.end))

    # Supressing help message of required args to overcome no sub-parser named groups
    subparser_cache = subparsers.add_parser(
        'cache',
        help = 'Cache remote resources locally.',
        usage = argparse.SUPPRESS,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description = required_cache_options,
        epilog = cache_epilog,
        add_help = False
    )

    # Required Arguments
    # Output Directory (analysis working directory)
    subparser_cache.add_argument(
        '--sif-cache',
        type = lambda option: os.path.abspath(os.path.expanduser(option)),
        required = True,
        help = argparse.SUPPRESS
    )

    # Optional Arguments
    # Dry-run cache command (do not pull any remote resources)
    subparser_cache.add_argument(
        '--dry-run',
        action = 'store_true',
        required = False,
        default = False,
        help=argparse.SUPPRESS
    )

    # Add custom help message
    subparser_cache.add_argument(
        '-h', '--help',
        action='help',
        help=argparse.SUPPRESS
    )

    # Define handlers for each sub-parser
    subparser_run.set_defaults(func = run)
    subparser_unlock.set_defaults(func = unlock)
    subparser_cache.set_defaults(func = cache)

    # Parse command-line args
    args = parser.parse_args()
    return args


def main():

    # Sanity check for usage
    if len(sys.argv) == 1:
        # Nothing was provided
        fatal('Invalid usage: {} [-h] [--version] ...'.format(_name))

    # Collect args for sub-command
    args = parsed_arguments(
        name = _name,
        description = _description
    )

    # Display version information
    err('{} ({})'.format(_name, __version__))

    # Mediator method to call sub-command's set handler function
    args.func(args)


if __name__ == '__main__':
    main()
